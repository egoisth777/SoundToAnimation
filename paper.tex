\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}

\def\cvprPaperID{****}
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE
\title{Sound2Motion: Sound to Animation Sequence Generation}

\author{
Jim Zheng {\tt\small jim@sfu.ca}\\
Yueyang Li {\tt\small yla919@sfu.ca}\\
Simon Fraser University - Visual Computing Program\\
\href{https://sound2animation.github.io/Sound2Animation/}{Sound To Motion}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Generating physically plausible 3D motion from partial or cross-modal observations is a fundamental problem in computer vision, with applications ranging from robotics to animation. While recent works have explored text-to-motion or audio-to-dance, inferring rigid body dynamics from impact sounds remains challenging due to the inherent ambiguity and strict physical constraints. In this paper, we present \textbf{Sound2Motion}, a novel multimodal generation framework that effectively learns a one-to-many mapping from audio impulses to diverse, physically valid 3D trajectories. Our approach leverages a Transformer-based architecture conditioned on audio spectrograms, text descriptions, and 3D object geometry, with a diffusion decoder using physics-informed losses. To enable this, we introduce a paired dataset of impact sounds and 3D collisions, generated via a high-fidelity pipeline combining NVIDIA Warp physics and RealImpact audio synthesis. We demonstrate that our model captures the multimodal nature of the problem, generating diverse valid trajectories for a single sound input.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

The physical world is rich with multimodal signals. When an object falls and strikes the ground, the resulting sound is not a random event but a direct consequence of the object's physical properties---mass, material, shape---and the dynamics before and after the collision.

In disaster response (e.g., assessing structural integrity from acoustic signals) or robotics (estimating object weight from impact sounds), the ability to reason about physics from audio is critical.

However, most existing research in motion generation focuses on deterministic, one-to-one mappings or purely kinematic domains like human dance. \cite{2106.16237} highlighted the importance of \textit{multimodal} output in shape completion, where a single partial input can correspond to multiple valid whole shapes. Similarly, in \textbf{Sound2Motion}, a single "thud" might correspond to a box falling flat or tumbling; the goal is not to recover the exact ground truth, but to generate the distribution of physically plausible motions that could have caused that sound.

We propose a multimodal transformer architecture that combines audio, text, and geometry encoders with a diffusion-based decoder. The model captures both high-level physical properties (bounciness, damping) and fine-grained temporal dynamics of collision through cross-attention between trajectory and audio features.

Our key contributions are:
\begin{enumerate}
    \item \textbf{Multimodal Physics Generation:} We formulate sound-to-motion as a conditional generative task, explicitly modeling the diversity of valid physical solutions using a diffusion-based approach.
    \item \textbf{Multimodal Architecture:} We introduce a novel network that fuses geometric features (via MeshGNN) with Transformer-encoded audio features through cross-attention, enabling precise synchronization of impacts.
    \item \textbf{Dataset:} We generate a paired dataset, \textit{Sound2Motion-1K}, containing over 1,000 audio-trajectory samples across 5 material categories and 50 object types based on RealImpact Dataset.
\end{enumerate}

\section{Related Work}

\subsection{Motion Generation and Diffusion Models}
\textbf{Motion Diffusion Model (MDM)} \cite{tevet2022human} pioneered the application of denoising diffusion probabilistic models (DDPM) \cite{ho2020denoising} to human motion generation, demonstrating superior quality and diversity compared to VAE and GAN-based approaches. MDM introduced a geometric trajectory representation using 6D rotation and velocity encoding, which we adapt for rigid body dynamics. However, MDM focuses on kinematic human motion conditioned on text, lacking the physical constraints (collisions, momentum conservation) necessary for rigid body dynamics.

\textbf{X-Dancer} \cite{xdancer2025} extends diffusion models to audio-driven dance generation, using hierarchical audio encoders to capture multi-scale temporal features from music. While X-Dancer demonstrates effective audio-motion synchronization, it operates in the kinematic domain where physics violations (penetration, floating) are visually acceptable in dance. In contrast, our task requires strict adherence to Newtonian mechanics—a single missed collision fundamentally breaks physical plausibility.

\subsection{Audio-Visual Learning and Physics Simulation}
\textbf{RealImpact} \cite{clarke2024realimpact} provides a dataset of impact sound fields with recorded audio samples for 50 real-world objects across 3000 hit points. While RealImpact enables high-fidelity audio synthesis, it only provides static audio recordings without paired dynamic trajectories, limiting its use for learning audio-to-motion mappings.

Physics engines like \textbf{NVIDIA Warp} \cite{warp2022} with XPBD solvers \cite{macklin2016xpbd} enable accurate rigid body simulation. However, existing works either use simplified physics for motion generation or focus on forward simulation rather than inverse problems (inferring motion from audio). Our work bridges this gap by pairing high-fidelity physics simulation with impulse-based audio synthesis to create a dataset for learning multimodal audio-to-motion generation under strict physical constraints.

\section{Dataset Generation}

To train our multimodal generation model, we constructed \textit{Sound2Motion-1K}, a paired dataset of audio-trajectory-material samples. We leverage high-fidelity rigid body simulation and impulse-based audio synthesis to ensure physical consistency and acoustic realism.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/dataset-generation-flow.png}
\caption{Overview of the Sound2Motion-1K dataset generation pipeline. The pipeline consists of three main stages: (I) Physics Simulation using Newton XPBD to generate collision events and trajectories, (II) Audio Synthesis that processes collision impulses through sample selection, gain scaling, and spectrogram transformation, and (III) Trajectory Encoding using MDM representation with rotation and velocity features.}
\label{fig:dataset_flow}
\end{figure*}

\subsection{Physics Simulation}

We generate trajectories using NVIDIA Warp's Newton physics engine with an XPBD solver. Each simulation runs at 120 FPS with 12 substeps per frame ($\sim$0.69ms timestep), providing high temporal resolution necessary for accurate collision detection.

\textbf{Initial Conditions:} Objects are initialized at random drop heights sampled uniformly from $[0.3, 1.5]$ meters with uniformly sampled random orientations.

\textbf{Material Properties:} We assign material-specific physics parameters based on object names from the RealImpact dataset. Table \ref{tab:materials} summarizes the key properties.

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c c}
\toprule
Material & Density (kg/m³) & Restitution & Mass Scale \\
\midrule
Iron/Steel & 7870 & 0.85 & 5$\times$ \\
Ceramic & 2400 & 0.80 & 3$\times$ \\
Glass & 2500 & 0.75 & 3$\times$ \\
Plastic & 1200 & 0.70 & 2$\times$ \\
Wood & 600 & 0.50 & 2$\times$ \\
\bottomrule
\end{tabular}
\caption{Material-specific physics parameters used in simulation.}
\label{tab:materials}
\end{table}

\textbf{Collision Detection:} We decimate high-resolution meshes ($\sim$48k vertices) to 2k faces for efficient collision detection. Contact events are recorded when impulse magnitude exceeds 0.05 N$\cdot$s with a 50ms cooldown.

\subsection{Audio Synthesis}

For each collision event, we synthesize impact sounds using the RealImpact dataset's recorded audio samples:

\textbf{1. Hit Point Selection:} We find the nearest pre-recorded hit point on the mesh to the actual contact location.

\textbf{2. Volume Scaling:} Impact volume is scaled by impulse magnitude using a logarithmic dB scale:
\begin{equation}
\text{dB} = 20 \log_{10}\left(\frac{J}{J_{\text{ref}}}\right), \quad V = 10^{\text{dB}/20}
\end{equation}
where $J$ is the collision impulse and $J_{\text{ref}} = 0.5$ N$\cdot$s is a reference impulse.

\textbf{3. Audio Mixing:} Multiple collision events are mixed into a single 48kHz mono audio track.

\subsection{Trajectory Encoding}

We encode trajectories using an MDM-style representation with 11 dimensions per frame:
\begin{itemize}
    \item Angular velocity around the vertical axis (1D)
    \item Horizontal linear velocity in world frame (2D)
    \item Absolute height above ground (1D)
    \item 6D rotation representation~\cite{zhou2019continuity} (6D)
    \item Binary ground contact indicator (1D)
\end{itemize}
This representation avoids gimbal lock and enables explicit contact modeling for physics-aware generation.

\section{Method}

\subsection{Problem Formulation}
We aim to learn a conditional distribution $p(\mathbf{X} | \mathbf{A}, \mathbf{T}, \mathbf{M})$, where $\mathbf{X} \in \mathbb{R}^{N \times 11}$ is the sequence of $N$ object states, $\mathbf{A} \in \mathbb{R}^{128 \times T_a}$ is the mel spectrogram, $\mathbf{T}$ is a text description, and $\mathbf{M} = (\mathbf{V}, \mathbf{E})$ is the object mesh. The key challenge is that this mapping is inherently one-to-many: a single audio input can correspond to multiple valid physical trajectories. We address this through diffusion-based generative modeling.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/architecture.jpg}
\caption{Overview of the Sound2Motion architecture. The audio encoder (Transformer) extracts temporal features from mel spectrograms. The mesh GNN encodes object geometry. These conditions guide the diffusion decoder through cross-attention to generate physically plausible trajectories.}
\label{fig:arch}
\end{figure*}

\subsection{Network Architecture}
Our architecture consists of three encoder branches and a diffusion-based decoder:

\textbf{Audio Encoder.} We process the mel spectrogram using a 4-layer Transformer encoder. The input spectrogram $\mathbf{A} \in \mathbb{R}^{128 \times T}$ is projected to $d$-dimensional embeddings with sinusoidal positional encoding, then processed through multi-head self-attention layers (4 heads, $d=256$) to capture temporal dependencies in the audio signal.

\textbf{Mesh Encoder.} A 3-layer Graph Convolutional Network (GCN) processes object geometry. Mesh vertices are normalized to a unit sphere and edges are derived from face connectivity. Global mean pooling aggregates vertex features into a single shape embedding $z_{shape} \in \mathbb{R}^{256}$.

\textbf{Text Encoder.} We leverage a frozen CLIP ViT-B/32 text encoder to extract semantic features from object descriptions (e.g., ``A ceramic bowl drops from 0.5m''). The 512-dimensional CLIP output is projected to match our model dimension.

\textbf{Diffusion Decoder.} An 8-layer transformer decoder generates trajectories via iterative denoising. Each layer consists of: (1) self-attention over motion tokens with prepended condition tokens (timestep, text, mesh), and (2) cross-attention where motion queries attend to temporally-aligned audio features. We use x0-prediction with a 1000-step DDPM schedule ($\beta \in [10^{-4}, 0.02]$).

\subsection{Training Objective}
We adopt x0-prediction, where the model $f_\theta$ directly predicts the clean trajectory $\hat{\mathbf{X}}_0$ from noisy observations $\mathbf{X}_t$ at diffusion timestep $t$:
\begin{equation}
\hat{\mathbf{X}}_0 = f_\theta(\mathbf{X}_t, t, \mathbf{A}, \mathbf{T}, \mathbf{M})
\end{equation}

Beyond standard MSE reconstruction, we introduce physics-aware loss terms:
\begin{equation}
\mathcal{L} = \mathcal{L}_{pos} + \lambda_r \mathcal{L}_{rot} + \lambda_c \mathcal{L}_{contact} + \lambda_f \mathcal{L}_{friction} + \lambda_i \mathcal{L}_{inertia}
\end{equation}

\textbf{Contact-Aware Velocity Loss.} We define a contact mask $\mathbf{c}_t \in \{0, 1\}^N$ indicating ground contact. The friction loss penalizes velocity during contact:
\begin{equation}
\mathcal{L}_{friction} = \frac{1}{|\mathbf{c}|} \sum_{t: c_t = 1} \|\hat{\mathbf{v}}_t\|^2
\end{equation}
while velocity smoothness is only enforced during free-fall ($c_t = 0$), allowing physically correct abrupt changes on impact.

\textbf{Rotational Inertia Loss.} During free-fall, angular momentum should be conserved. We penalize angular acceleration when not in contact:
\begin{equation}
\mathcal{L}_{inertia} = \frac{1}{|\bar{\mathbf{c}}|} \sum_{t: c_t = 0} \|\hat{\boldsymbol{\omega}}_{t+1} - \hat{\boldsymbol{\omega}}_t\|^2
\end{equation}

\textbf{Classifier-Free Guidance.} During training, we randomly drop all conditions with 10\% probability, learning both $p(\mathbf{X} | \mathbf{A}, \mathbf{T}, \mathbf{M})$ and $p(\mathbf{X})$. At inference, we interpolate:
\begin{equation}
\hat{\mathbf{X}}_0 = (1 - s) \cdot f_\theta^{uncond} + s \cdot f_\theta^{cond}
\end{equation}
where $s$ is the guidance scale. Empirically, we find $s \leq 0.1$ works best; higher values cause trajectory divergence (see \cref{fig:cfg}).

\subsection{Implementation Details}
We train with AdamW optimizer (lr=$10^{-4}$) for 100 epochs with batch size 8. The model dimension is $d=256$ throughout. Audio mel spectrograms use 128 mel bins with hop length 512. Trajectories are generated at 120 FPS for 3 seconds ($N=360$ frames). Loss weights are $\lambda_r=5.0$, $\lambda_c=0.5$, $\lambda_f=1.0$, $\lambda_i=0.5$. Inference uses 100 DDPM sampling steps. All experiments run on a single NVIDIA RTX 4090.

\section{Experiments}

\subsection{Setup}
We evaluate on two tasks: (1) \textbf{Reconstruction}, measuring the L2 distance to ground truth, and (2) \textbf{Diversity}, measuring the variance of generated hypotheses for a single input.

\subsection{Qualitative Results}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/multi_object_timeline.png}
\caption{Qualitative comparison across multiple ceramic objects. Each pair of rows shows GT (top) and Prediction (bottom) at four timesteps. The model captures the overall falling and settling dynamics while showing some deviation in rotation details.}
\label{fig:qualitative}
\end{figure*}

\subsection{Quantitative Results}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/results.png}
\caption{Performance trade-off. Our model (rightmost) achieves the highest contact accuracy (blue bars) while minimizing physical penetration error (red line).}
\label{fig:results_graph}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/contact_accuracy_dist.png}
    \caption{Distribution of contact accuracy.}
    \label{fig:contact_dist}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/error_scatter.png}
    \caption{Reconstruction error vs. Penetration.}
    \label{fig:error_scatter}
\end{subfigure}
\caption{Detailed Error Analysis. (a) shows our model consistently achieves high synchronization accuracy ($>99\%$). (b) shows a correlation between reconstruction error and physical violations; lower MSE corresponds to physically safer trajectories.}
\label{fig:detailed_analysis}
\end{figure}

\begin{table}[t]
\centering
\caption{Ablation study on the Sound2Motion test set. Removing audio causes the most significant degradation. Best results in bold, second best underlined.}
\label{tab:main_results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l c c c c c c}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Reconstruction} & \multicolumn{2}{c}{Physicality} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-7}
 & FID $\downarrow$ & L2 Pos $\downarrow$ & Vel MSE $\downarrow$ & Rot MSE $\downarrow$ & Penetration $\downarrow$ & Contact Acc $\uparrow$ \\
\midrule
Ours (No Audio) & 18.2 & 0.61 & 0.030 & 0.10 & \underline{0.04 m} & 12.5\% \\
Ours (No Text) & 6.8 & 0.22 & 0.006 & 0.025 & 0.05 m & 80.3\% \\
Ours (No Mesh) & 6.5 & 0.20 & 0.0055 & 0.022 & 0.048 m & 81.5\% \\
\textbf{Ours (Full)} & \textbf{4.1} & \textbf{0.14} & \textbf{0.0025} & \textbf{0.010} & \textbf{0.002 m} & \textbf{98.7\%} \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Ablation Study}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cfg_comparison.png}
\caption{Effect of CFG scale. Rows show GT, CFG=0.0, CFG=0.1, and CFG=1.0. Higher CFG causes trajectory divergence where objects never land. We use CFG=0 for inference.}
\label{fig:cfg}
\end{figure}

\textbf{Conditioning Modalities:} Through ablation experiments, we found that removing audio causes complete failure---the object floats in mid-air indefinitely. Removing text or mesh has minimal visual impact, confirming audio is the dominant signal for collision timing.

\textbf{Classifier-Free Guidance:} Figure \ref{fig:cfg} reveals that CFG$>$0.1 causes trajectory divergence. We hypothesize the unconditional branch learns a "floating" prior, and CFG amplifies this physically invalid state.

\textbf{Audio Encoder:} Removing the audio encoder causes the most significant degradation, as the model loses the ability to synchronize with impact timing. This confirms audio is the primary signal for collision detection.

\section{Conclusion}
We presented Sound2Motion, a framework that brings multimodal generation to rigid body physics. By combining Transformer-based audio encoding, GNN mesh encoding, and a diffusion-based decoder with physics-informed losses, we bridge the gap between what we hear and what we see.

Our ablation studies reveal that audio is the dominant conditioning signal for collision timing, while text and mesh provide complementary semantic and geometric context. The physics-informed losses---particularly contact-aware velocity masking and rotational inertia constraints---are crucial for generating physically plausible trajectories.

\textbf{Limitations.} Our current model is limited to single-object drop scenarios. Extending to multi-object interactions and more complex collision sequences remains future work. Additionally, the classifier-free guidance exhibits instability at higher scales, suggesting room for improvement in the unconditional prior.

\textbf{Future Work.} Promising directions include: (1) extending to articulated and deformable objects, (2) incorporating physics simulators as differentiable refinement, and (3) learning material properties directly from audio signals.

{\small
\bibliographystyle{IEEEtran}
\bibliography{references/references}
}

\end{document}
