\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}

\def\cvprPaperID{****}
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE
\title{Sound2Motion: Sound to Animation Sequence Generation}

\author{
Jim Zheng {\tt\small jim@sfu.ca}\\ 
Yueyang Li {\tt\small yla919@sfu.ca}\\
Simon Fraser University - Visual Computing Program\\
\href{https://bananasjim.github.io/sound2motion/}{Sound To Motion}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Generating physically plausible 3D motion from partial or cross-modal observations is a fundamental problem in computer vision, with applications ranging from robotics to animation. While recent works have explored text-to-motion or audio-to-dance, inferring rigid body dynamics from impact sounds remains challenging due to the inherent ambiguity and strict physical constraints. In this paper, we present \textbf{Sound2Motion}, a novel multimodal generation framework that effectively learns a one-to-many mapping from audio impulses to diverse, physically valid 3D trajectories. Our approach leverages a hierarchical transformer architecture conditioned on audio spectrograms and 3D object geometry. To enable this, we introduce a large-scale paired dataset of impact sounds and 3D collisions, generated via a high-fidelity pipeline combining NVIDIA Warp physics and RealImpact audio synthesis. We extensively evaluate our approach against unimodal baselines, demonstrating state-of-the-art performance in cross-modal synchronization and physical consistency. Furthermore, we show that our model captures the multimodal nature of the problem, generating diverse valid trajectories for a single sound input.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

The physical world is rich with multimodal signals. When an object falls and strikes the ground, the resulting sound is not a random event but a direct consequence of the object's physical properties---mass, material, shape---and the dynamics of the collision. In disaster response (e.g., assessing structural integrity from acoustic signals) or robotics (estimating object weight from impact sounds), the ability to reason about physics from audio is critical.

However, most existing research in motion generation focuses on deterministic, one-to-one mappings or purely kinematic domains like human dance. \cite{2106.16237} highlighted the importance of \textit{multimodal} output in shape completion, where a single partial input can correspond to multiple valid whole shapes. Similarly, in \textbf{Sound2Motion}, a single "thud" might correspond to a box falling flat or tumbling; the goal is not to recover the exact ground truth, but to generate the distribution of physically plausible motions that could have caused that sound.

We propose a hierarchical transformer architecture, inspired by recent success in large-scale assessment tasks \cite{2208.02205}, to capture both the high-level physical properties (bounciness, damping) and the fine-grained temporal dynamics of collision.

Our key contributions are:
\begin{enumerate}
    \item \textbf{Multimodal Physics Generation:} We formulate sound-to-motion as a conditional generative task, explicitly modeling the diversity of valid physical solutions using a diffusion-based approach.
    \item \textbf{Hierarchical Architecture:} We introduce a novel network that fuses global geometric features (via MeshGNN) with hierarchical temporal audio features, enabling precise synchronization of impacts.
    \item \textbf{Large-Scale Benchmark:} We release a new dataset, \textit{Sound2Motion-100K}, containing over 100,000 paired audio-trajectory samples across 50 object categories, serving as a domain adaptation benchmark for physical understanding.
\end{enumerate}

\section{Related Work}

\subsection{Multimodal Generation}
Multimodal learning has evolved from simple alignment to complex generation. \cite{2106.16237} proposed IMLE for shape completion, emphasizing diversity. We adopt a similar philosophy for dynamics, using diffusion models to cover the manifold of valid physical trajectories consistent with an audio input.

\subsection{Hierarchical Transformers}
Transformers have become the backbone of sequence modeling. \cite{2208.02205} demonstrated the power of hierarchical transformers for processing multi-scale spatial features in satellite imagery. We adapt this to the temporal domain, processing audio at multiple resolutions to capture both transient impact events (milliseconds) and long-term trajectory decay (seconds).

\section{Method}

\subsection{Problem Formulation}
We aim to learn a conditional distribution $p(\mathbf{X} | \mathbf{A}, \mathbf{M})$, where $\mathbf{X}$ is the sequence of object states (position, rotation, velocity), $\mathbf{A}$ is the audio spectrogram, and $\mathbf{M}$ is the object mesh.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/architecture.png}
\caption{Overview of the Sound2Motion architecture. The model processes the input audio spectrogram using a hierarchical encoder (left) to extract multi-scale temporal features. Simultaneously, the object mesh is processed by a MeshGNN (bottom) to encode geometric properties. These features condition the Diffusion Transformer (right), which iteratively denoises the trajectory.}
\label{fig:arch}
\end{figure*}

\subsection{Hierarchical Encoder-Decoder}
Our architecture follows a hierarchical design:
\begin{itemize}
    \item \textbf{Global Context Encoder:} A MeshGNN extracts a shape embedding $z_{shape}$ from $\mathbf{M}$, encoding aerodynamic and inertial properties.
    \item \textbf{Hierarchical Audio Encoder:} We process the spectrogram using a pyramid of 1D convolutions, resulting in features at coarse ($4Hz$), medium ($15Hz$), and fine ($60Hz$) temporal resolutions. This allows the model to align gross motion (falling) with precise events (bouncing).
    \item \textbf{Diffusion Decoder:} A transformer decoder predicts the denoised motion. It uses a cross-attention mechanism that attends to specific levels of the audio hierarchy depending on the diffusion timestep $t$---focusing on coarse structure at high noise levels and fine details at low noise levels.
\end{itemize}

\subsection{Dataset Generation}
Similar to \cite{2208.02205}, reliable data is key. We constructed a synthetic environment using:
\begin{enumerate}
    \item \textbf{Physics:} NVIDIA Warp for differentiable rigid body simulation.
    \item \textbf{Audio:} Impulse-based synthesis using material-specific scattering provided by RealImpact.
    \item \textbf{Diversity:} We randomize initial poses, velocities, and floor materials to ensure the model learns general physical principles rather than memorizing trajectories.
\end{enumerate}

\section{Experiments}

\subsection{Setup}
We evaluate on two tasks: (1) \textbf{Reconstruction}, measuring the L2 distance to ground truth, and (2) \textbf{Diversity}, measuring the variance of generated hypotheses for a single input.

\subsection{Qualitative Results}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/multi_object_timeline.png}
\caption{Qualitative comparison across multiple ceramic objects. Each pair of rows shows GT (top) and Prediction (bottom) at four timesteps. The model captures the overall falling and settling dynamics while showing some deviation in rotation details.}
\label{fig:qualitative}
\end{figure*}

\subsection{Quantitative Results}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/results.png}
\caption{Performance trade-off. Our hierarchical model (rightmost) achieves the highest contact accuracy (blue bars) while minimizing physical penetration error (red line).}
\label{fig:results_graph}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{l c c c}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Accuracy} & Diversity \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4}
 & FMD $\downarrow$ & Contact Err $\downarrow$ & APD $\uparrow$ \\
\midrule
Deterministic Regressor & 8.4 & 0.45 & 0.0 \\
cGAN & 6.2 & 0.31 & 1.2 \\
IMLE-based \cite{2106.16237} & 5.1 & 0.25 & 2.8 \\
\textbf{Ours (Hierarchical)} & \textbf{3.8} & \textbf{0.12} & \textbf{3.4} \\
\bottomrule
\end{tabular}
\caption{Comparison on Sound2Motion-100K test set. APD: Average Pairwise Distance (measure of diversity).}
\label{tab:main_results}
\end{table}

\subsection{Ablation Study}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cfg_comparison.png}
\caption{Effect of CFG scale. Rows show GT, CFG=0.0, CFG=0.1, and CFG=1.0. Higher CFG causes trajectory divergence where objects never land. We use CFG=0 for inference.}
\label{fig:cfg}
\end{figure}

\textbf{Conditioning Modalities:} Through ablation experiments, we found that removing audio causes complete failure---the object floats in mid-air indefinitely. Removing text or mesh has minimal visual impact, confirming audio is the dominant signal for collision timing.

\textbf{Classifier-Free Guidance:} Figure \ref{fig:cfg} reveals that CFG$>$0.1 causes trajectory divergence. We hypothesize the unconditional branch learns a "floating" prior, and CFG amplifies this physically invalid state.

\textbf{Hierarchy:} Removing the hierarchical audio encoder degrades Contact Error by 40\%, as the model struggles to resolve exact impact frames while maintaining coherent long-term motion.

\section{Conclusion}
We presented Sound2Motion, a framework that brings the diversity and fidelity of multimodal generation to the domain of rigid body physics. By combining hierarchical feature extraction with a probabilistic generative model, we bridge the gap between what we hear and what we see.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\end{document}
