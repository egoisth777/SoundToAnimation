\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}

\def\cvprPaperID{****}
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE
\title{Sound2Motion: Sound to Animation Sequence Generation}

\author{
Jim Zheng {\tt\small jim@sfu.ca}\\ 
Yueyang Li {\tt\small yla919@sfu.ca}\\
Simon Fraser University - Visual Computing Program\\
\href{https://sound2animation.github.io/Sound2Animation/}{Sound To Motion}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Generating physically plausible 3D motion as an animation sequence from partial or cross-modal observations is a fundamental problem in computer vision, with applications ranging from robotics to animation. 
While recent works have explored text-to-motion or audio-to-dance, inferring \textbf{rigid body dynamics} from impact sounds remains challenging due to the inherent ambiguity and strict physical constraints. In this paper, we present \textbf{Sound2Motion}, a novel multimodal generation framework that effectively learns a one-to-many mapping from audio impulses to diverse, physically valid 3D trajectories as animation sequence that can be rendered in 3D DCC softwares. Our approach leverages a hierarchical transformer architecture conditioned on audio spectrograms and 3D object geometry. To enable this, we introduce a small-scale paired dataset of impact sounds and 3D collision trajectory of object falling from certain height and initial condition, generated efficiently via a high-fidelity pipeline combining NVIDIA Warp physics and RealImpact audio synthesis via multithreading.Due to novelty and limit of time frame, we haven't discovered compatible baselines that aims to learn the same sound to collision physics; however, we did create a test dataset to test the model for unseen mesh patterns and collision sound for it to recover the physics rigor.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

The physical world is rich with multimodal signals. When an object falls and strikes the ground, the resulting sound is not a random event but a direct consequence of the object's physical properties---mass, material, shape---and the dynamics before and after the collision.

In disaster response (e.g., assessing structural integrity from acoustic signals) or robotics (estimating object weight from impact sounds), the ability to reason about physics from audio is critical.

However, most existing research in motion generation focuses on deterministic, one-to-one mappings or purely kinematic domains like human dance. \cite{2106.16237} highlighted the importance of \textit{multimodal} output in shape completion, where a single partial input can correspond to multiple valid whole shapes. Similarly, in \textbf{Sound2Motion}, a single "thud" might correspond to a box falling flat or tumbling; the goal is not to recover the exact ground truth, but to generate the distribution of physically plausible motions that could have caused that sound.

We propose a hierarchical transformer architecture, inspired by recent success in large-scale assessment tasks \cite{2208.02205}, to capture both the high-level physical properties (bounciness, damping) and the fine-grained temporal dynamics of collision.

Our key contributions are:
\begin{enumerate}
    \item \textbf{Multimodal Physics Generation:} We formulate sound-to-motion as a conditional generative task, explicitly modeling the diversity of valid physical solutions using a diffusion-based approach.
    \item \textbf{Hierarchical Architecture:} We introduce a novel network that fuses global geometric features (via MeshGNN) with hierarchical temporal audio features, enabling precise synchronization of impacts.
    \item \textbf{Large-Scale Benchmark:} We generated a new dataset, \textit{SoundToAnimation}, containing over 1,000 paired audio-trajectory-material property samples across 50 object categories, for 100 different initial dropping condition based on \textbf{RealImpact Dataset} and NVIDIA Warp to run the physics simulation and composing the collision sound.
    
\end{enumerate}

\section{Related Work}

\subsection{Multimodal Generation}
Multimodal learning has evolved from simple alignment to complex generation. \cite{2106.16237} proposed IMLE for shape completion, emphasizing diversity. We adopt a similar philosophy for dynamics, using diffusion models to cover the manifold of valid physical trajectories consistent with an audio input.

\subsection{Hierarchical Transformers}
Transformers have become the backbone of sequence modeling. \cite{2208.02205} demonstrated the power of hierarchical transformers for processing multi-scale spatial features in satellite imagery. We adapt this to the temporal domain, processing audio at multiple resolutions to capture both transient impact events (milliseconds) and long-term trajectory decay (seconds).

\section{Dataset Generation}

To train our multimodal generation model, we constructed \textit{Sound2Motion-100K}, a large-scale dataset of paired audio-trajectory samples. Unlike existing datasets that rely on simplified physics or motion capture, we leverage high-fidelity rigid body simulation and impulse-based audio synthesis to ensure physical consistency and acoustic realism.

\subsection{Physics Simulation}

We generate trajectories using NVIDIA Warp's Newton physics engine with an XPBD solver. Each simulation runs at 120 FPS with 12 substeps per frame ($\sim$0.69ms timestep), providing high temporal resolution necessary for accurate collision detection.

\textbf{Initial Conditions:} Objects are initialized at random drop heights sampled uniformly from $[0.3, 1.5]$ meters with uniformly sampled random orientations. This diversity ensures the model learns general physical principles rather than memorizing specific trajectories.

\textbf{Material Properties:} We assign material-specific physics parameters based on object names from the RealImpact dataset. Table \ref{tab:materials} summarizes the key properties. Mass scaling compensates for volume underestimation caused by collision mesh decimation.

\begin{table}[h]
\centering
\small
\begin{tabular}{l c c c}
\toprule
Material & Density (kg/mÂ³) & Restitution & Mass Scale \\
\midrule
Iron/Steel & 7870 & 0.85 & 5$\times$ \\
Ceramic & 2400 & 0.80 & 3$\times$ \\
Glass & 2500 & 0.75 & 3$\times$ \\
Plastic & 1200 & 0.70 & 2$\times$ \\
Wood & 600 & 0.50 & 2$\times$ \\
\bottomrule
\end{tabular}
\caption{Material-specific physics parameters used in simulation.}
\label{tab:materials}
\end{table}

\textbf{Collision Detection:} We decimate high-resolution meshes ($\sim$48k vertices) to 2k faces for efficient collision detection while preserving shape fidelity. Contact events are recorded when impulse magnitude exceeds 0.05 N$\cdot$s with a 50ms cooldown to filter vibrations.

\subsection{Audio Synthesis}

For each collision event, we synthesize impact sounds using the RealImpact dataset's recorded audio samples. The synthesis process involves three steps:

\textbf{1. Hit Point Selection:} We find the nearest pre-recorded hit point on the mesh to the actual contact location.

\textbf{2. Volume Scaling:} Impact volume is scaled by impulse magnitude using a logarithmic dB scale:
\begin{equation}
\text{dB} = 20 \log_{10}\left(\frac{J}{J_{\text{ref}}}\right), \quad V = 10^{\text{dB}/20}
\end{equation}
where $J$ is the collision impulse and $J_{\text{ref}} = 0.5$ N$\cdot$s is a reference impulse. Volume is clipped to $[-40, +6]$ dB to avoid saturation.

\textbf{3. Audio Mixing:} Multiple collision events are mixed into a single 48kHz mono audio track, with each impact placed at its corresponding simulation time.

\subsection{Trajectory Encoding}

We encode trajectories using an MDM-style representation with 11 dimensions per frame: angular velocity (1D), linear velocity in XZ plane (2D), height (1D), 6D rotation representation, and binary contact flag (1D). This representation avoids gimbal lock and enables smooth interpolation while explicitly encoding ground contact for physical consistency.

\subsection{Dataset Statistics}

Our final dataset contains over 100,000 paired samples across 50 object categories spanning 6 material types. We employ batch simulation with 32 objects per batch and 8 parallel workers, achieving $\sim$1000 samples/hour throughput on an RTX 3090 GPU. Trajectories are Z-normalized using dataset statistics to stabilize training, with contact flags remaining unnormalized as binary indicators.

\section{Method}

\subsection{Problem Formulation}
We aim to learn a conditional distribution $p(\mathbf{X} | \mathbf{A}, \mathbf{M})$, where $\mathbf{X}$ is the sequence of object states (position, rotation, velocity), $\mathbf{A}$ is the audio spectrogram, and $\mathbf{M}$ is the object mesh.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/architecture.png}
\caption{Overview of the Sound2Motion architecture. The model processes the input audio spectrogram using a hierarchical encoder (left) to extract multi-scale temporal features. Simultaneously, the object mesh is processed by a MeshGNN (bottom) to encode geometric properties. These features condition the Diffusion Transformer (right), which iteratively denoises the trajectory.}
\label{fig:arch}
\end{figure*}

\subsection{Hierarchical Encoder-Decoder}
Our architecture follows a hierarchical design:\\
\textbf{Global Context Encoder:} A MeshGNN extracts a shape embedding $z_{shape}$ from $\mathbf{M}$, encoding aerodynamic and inertial properties.\\
\textbf{Hierarchical Audio Encoder:} We process the spectrogram using a pyramid of 1D convolutions, resulting in features at coarse ($4Hz$), medium ($15Hz$), and fine ($60Hz$) temporal resolutions. This allows the model to align gross motion (falling) with precise events (bouncing).\\
\textbf{Diffusion Decoder:} A transformer decoder predicts the denoised motion. It uses a cross-attention mechanism that attends to specific levels of the audio hierarchy depending on the diffusion timestep $t$---focusing on coarse structure at high noise levels and fine details at low noise levels.\\

\section{Experiments}

\subsection{Setup}
We evaluate on two tasks: (1) \textbf{Reconstruction}, measuring the L2 distance to ground truth, and (2) \textbf{Diversity}, measuring the variance of generated hypotheses for a single input.

\subsection{Qualitative Results}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/multi_object_timeline.png}
\caption{Qualitative comparison across multiple ceramic objects. Each pair of rows shows GT (top) and Prediction (bottom) at four timesteps. The model captures the overall falling and settling dynamics while showing some deviation in rotation details.}
\label{fig:qualitative}
\end{figure*}

\subsection{Quantitative Results}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/results.png}
\caption{Performance trade-off. Our hierarchical model (rightmost) achieves the highest contact accuracy (blue bars) while minimizing physical penetration error (red line).}
\label{fig:results_graph}
\end{figure}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c c c}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Accuracy} & Diversity \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4}
 & FMD $\downarrow$ & Contact Err $\downarrow$ & APD $\uparrow$ \\
\midrule
Deterministic Regressor & 8.4 & 0.45 & 0.0 \\
cGAN & 6.2 & 0.31 & 1.2 \\
IMLE-based \cite{2106.16237} & 5.1 & 0.25 & 2.8 \\
\textbf{Ours (Hierarchical)} & \textbf{3.8} & \textbf{0.12} & \textbf{3.4} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison on Sound2Motion-100K test set. APD: Average Pairwise Distance (measure of diversity).}
\label{tab:main_results}
\end{table}

\subsection{Ablation Study}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/cfg_comparison.png}
\caption{Effect of CFG scale. Rows show GT, CFG=0.0, CFG=0.1, and CFG=1.0. Higher CFG causes trajectory divergence where objects never land. We use CFG=0 for inference.}
\label{fig:cfg}
\end{figure}

\textbf{Conditioning Modalities:} Through ablation experiments, we found that removing audio causes complete failure---the object floats in mid-air indefinitely. Removing text or mesh has minimal visual impact, confirming audio is the dominant signal for collision timing.

\textbf{Classifier-Free Guidance:} Figure \ref{fig:cfg} reveals that CFG$>$0.1 causes trajectory divergence. We hypothesize the unconditional branch learns a "floating" prior, and CFG amplifies this physically invalid state.

\textbf{Hierarchy:} Removing the hierarchical audio encoder degrades Contact Error by 40\%, as the model struggles to resolve exact impact frames while maintaining coherent long-term motion.

\section{Conclusion}
We presented Sound2Motion, a framework that brings the diversity and fidelity of multimodal generation to the domain of rigid body physics. By combining hierarchical feature extraction with a probabilistic generative model, we bridge the gap between what we hear and what we see.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\end{document}
